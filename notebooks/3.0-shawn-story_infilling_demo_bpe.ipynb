{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TinyStories Story Infilling Model Demo with BPE Tokenizer\n",
    "\n",
    "This notebook demonstrates how to use the TinyStories story infilling model with the BPE tokenizer. The model takes the first and last sentences of a story as input and generates the middle part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shawn/Library/Caches/pypoetry/virtualenvs/tinystoriesproject-PsthS1z7-py3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project path: /Users/shawn/Documents/sjsu/2025-1/DL_CMPE258/TinyStoriesProject\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "# Add project directory to path to import modules\n",
    "current_path = os.path.abspath('.')\n",
    "project_name = 'TinyStoriesProject'\n",
    "project_path = os.path.join(current_path.split(project_name)[0], project_name)\n",
    "sys.path.append(project_path)\n",
    "print(f\"Project path: {project_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import project modules\n",
    "from src.models import StoryInfillingModel\n",
    "from src.bpe_tokenizer import BPETokenizerWrapper\n",
    "from src.generate_story import generate_story"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the pre-trained model\n",
    "\n",
    "First, let's load the pre-trained model from the saved checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if model exists\n",
    "model_path = os.path.join(project_path, 'model', 'tinystories_bpe_infilling_model.pth')\n",
    "if not os.path.exists(model_path):\n",
    "    print(\"Model file not found. You need to train the model first by running src/train_infilling_model.py\")\n",
    "else:\n",
    "    print(f\"Model found at {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model checkpoint\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "model_args = checkpoint['args']\n",
    "tokenizer_model_name = checkpoint.get('tokenizer_model', 'gpt2')\n",
    "\n",
    "print(f\"Model was trained for {checkpoint['epoch']} epochs\")\n",
    "print(f\"Train loss: {checkpoint['train_loss']:.4f}, Validation loss: {checkpoint['valid_loss']:.4f}\")\n",
    "print(f\"Using tokenizer model: {tokenizer_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BPE tokenizer\n",
    "tokenizer = BPETokenizerWrapper(\n",
    "    model_name=tokenizer_model_name,\n",
    "    special_tokens={\"blank_token\": \"<blank>\"}\n",
    ")\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "print(f\"Tokenizer vocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = StoryInfillingModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=model_args['embed_dim'],\n",
    "    num_layers=model_args['num_layers'],\n",
    "    num_heads=model_args['num_heads'],\n",
    "    ff_dim=model_args['ff_dim'],\n",
    "    max_seq_length=model_args['max_seq_length'],\n",
    "    dropout=model_args['dropout'],\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    blank_token_id=tokenizer.blank_token_id\n",
    ").to(device)\n",
    "\n",
    "# Load model weights\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Testing with examples from the validation set\n",
    "\n",
    "Let's load the validation set and test our model with some real examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load validation dataset\n",
    "valid_dataset = load_dataset(\"roneneldan/TinyStories\", split=\"validation\")\n",
    "print(f\"Validation dataset loaded with {len(valid_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_first_last_sentences(text):\n",
    "    \"\"\"Extract the first and last sentences from a story.\"\"\"\n",
    "    # Simple sentence splitting by period\n",
    "    sentences = text.split('.')\n",
    "    sentences = [s.strip() + '.' for s in sentences if s.strip()]\n",
    "    \n",
    "    if len(sentences) < 2:\n",
    "        return None, None\n",
    "    \n",
    "    return sentences[0], sentences[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a random example from the validation set\n",
    "def test_with_random_example():\n",
    "    # Get a random example\n",
    "    idx = random.randint(0, len(valid_dataset) - 1)\n",
    "    story = valid_dataset[idx]['text']\n",
    "    \n",
    "    # Extract first and last sentences\n",
    "    first_sentence, last_sentence = extract_first_last_sentences(story)\n",
    "    \n",
    "    if not first_sentence or not last_sentence:\n",
    "        print(\"Couldn't extract sentences properly. Trying another example.\")\n",
    "        return test_with_random_example()\n",
    "    \n",
    "    print(\"Original story:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(story)\n",
    "    print(\"-\" * 80)\n",
    "    print()\n",
    "    \n",
    "    print(\"First sentence:\")\n",
    "    print(first_sentence)\n",
    "    print()\n",
    "    \n",
    "    print(\"Last sentence:\")\n",
    "    print(last_sentence)\n",
    "    print()\n",
    "    \n",
    "    # Generate the middle part with our model\n",
    "    generated_story = model.generate(\n",
    "        first_sentence,\n",
    "        last_sentence,\n",
    "        tokenizer,\n",
    "        max_length=150,\n",
    "        teacher_forcing_ratio=0.0  # During testing, we don't use teacher forcing\n",
    "    )\n",
    "    \n",
    "    print(\"Generated story:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(generated_story)\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    return first_sentence, last_sentence, story, generated_story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sentence, last_sentence, original_story, generated_story = test_with_random_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Custom examples\n",
    "\n",
    "Now let's try with our own custom first and last sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_custom_input(first_sentence, last_sentence, max_tokens=150):\n",
    "    print(\"First sentence:\")\n",
    "    print(first_sentence)\n",
    "    print()\n",
    "    \n",
    "    print(\"Last sentence:\")\n",
    "    print(last_sentence)\n",
    "    print()\n",
    "    \n",
    "    # Generate the middle part with our model\n",
    "    generated_story = model.generate(\n",
    "        first_sentence,\n",
    "        last_sentence,\n",
    "        tokenizer,\n",
    "        max_length=max_tokens,\n",
    "        teacher_forcing_ratio=0.0\n",
    "    )\n",
    "    \n",
    "    print(\"Generated story:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(generated_story)\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    return generated_story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1\n",
    "custom_first_1 = \"Once upon a time, there was a little boy named Tim who loved to play with toys.\"\n",
    "custom_last_1 = \"Tim learned that sharing his toys made everyone happy, including himself.\"\n",
    "\n",
    "generated_story_1 = generate_with_custom_input(custom_first_1, custom_last_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2\n",
    "custom_first_2 = \"Sarah was excited to visit the zoo with her family on Saturday.\"\n",
    "custom_last_2 = \"They all agreed it was the best day ever and couldn't wait to come back.\"\n",
    "\n",
    "generated_story_2 = generate_with_custom_input(custom_first_2, custom_last_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3\n",
    "custom_first_3 = \"It was a rainy day and Max was feeling sad because he couldn't go outside to play.\"\n",
    "custom_last_3 = \"Max realized that rainy days could be fun too.\"\n",
    "\n",
    "generated_story_3 = generate_with_custom_input(custom_first_3, custom_last_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Experiment with generation parameters\n",
    "\n",
    "Let's try changing the generation parameters to see how they affect the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_parameters(first_sentence, last_sentence, max_tokens=150, temperature=1.0, top_k=50, top_p=0.9):\n",
    "    # Initialize model for story generation with different parameters\n",
    "    # Temperature controls randomness: higher values (>1.0) make output more random, lower values (<1.0) make it more deterministic\n",
    "    \n",
    "    print(f\"Generating with temperature={temperature}, top_k={top_k}, top_p={top_p}\")\n",
    "    \n",
    "    # For direct use of the model's generate method with more control\n",
    "    generated_story = model.generate(\n",
    "        first_sentence,\n",
    "        last_sentence,\n",
    "        tokenizer,\n",
    "        max_length=max_tokens,\n",
    "        temperature=temperature,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,\n",
    "        teacher_forcing_ratio=0.0\n",
    "    )\n",
    "    \n",
    "    print(\"Generated story:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(generated_story)\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    return generated_story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose an example\n",
    "test_first = \"Jake was a little boy who always wanted a puppy for his birthday.\"\n",
    "test_last = \"Jake was so happy with his new puppy and promised to take care of it forever.\"\n",
    "\n",
    "print(\"First sentence:\")\n",
    "print(test_first)\n",
    "print()\n",
    "print(\"Last sentence:\")\n",
    "print(test_last)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate with different temperatures\n",
    "low_temp_story = generate_with_parameters(test_first, test_last, temperature=0.5)\n",
    "normal_temp_story = generate_with_parameters(test_first, test_last, temperature=1.0)\n",
    "high_temp_story = generate_with_parameters(test_first, test_last, temperature=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate with different top_k and top_p values\n",
    "low_k_story = generate_with_parameters(test_first, test_last, top_k=10)\n",
    "high_k_story = generate_with_parameters(test_first, test_last, top_k=100)\n",
    "low_p_story = generate_with_parameters(test_first, test_last, top_p=0.5)\n",
    "high_p_story = generate_with_parameters(test_first, test_last, top_p=0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Trying teacher forcing during generation\n",
    "\n",
    "Let's see how teacher forcing affects generation when we have a ground truth story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_teacher_forcing(first_sentence, last_sentence, ground_truth, ratio=0.5):\n",
    "    print(f\"Generating with teacher_forcing_ratio={ratio}\")\n",
    "    print(\"Ground truth:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(ground_truth)\n",
    "    print(\"-\" * 80)\n",
    "    print()\n",
    "    \n",
    "    # Generate with teacher forcing\n",
    "    generated_story = model.generate(\n",
    "        first_sentence,\n",
    "        last_sentence,\n",
    "        tokenizer,\n",
    "        max_length=200,\n",
    "        teacher_forcing_ratio=ratio,\n",
    "        ground_truth=ground_truth\n",
    "    )\n",
    "    \n",
    "    print(\"Generated story:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(generated_story)\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    return generated_story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a random story from the validation set\n",
    "idx = random.randint(0, len(valid_dataset) - 1)\n",
    "ground_truth = valid_dataset[idx]['text']\n",
    "first_sentence, last_sentence = extract_first_last_sentences(ground_truth)\n",
    "\n",
    "if not first_sentence or not last_sentence:\n",
    "    print(\"Couldn't extract sentences properly. Please run this cell again.\")\n",
    "else:\n",
    "    # Generate with different teacher forcing ratios\n",
    "    no_tf_story = generate_with_teacher_forcing(first_sentence, last_sentence, ground_truth, ratio=0.0)\n",
    "    half_tf_story = generate_with_teacher_forcing(first_sentence, last_sentence, ground_truth, ratio=0.5)\n",
    "    full_tf_story = generate_with_teacher_forcing(first_sentence, last_sentence, ground_truth, ratio=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Continuation Generation (Without Last Sentence)\n",
    "\n",
    "The model can also generate continuations when only given the first sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_continuation(first_sentence, max_tokens=200):\n",
    "    print(\"First sentence:\")\n",
    "    print(first_sentence)\n",
    "    print()\n",
    "    \n",
    "    # Generate continuation with our model\n",
    "    generated_story = model.generate(\n",
    "        first_sentence,\n",
    "        last_sentence=None,  # No last sentence for continuation\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=max_tokens,\n",
    "        teacher_forcing_ratio=0.0\n",
    "    )\n",
    "    \n",
    "    print(\"Generated continuation:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(generated_story)\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    return generated_story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example continuations\n",
    "custom_first_1 = \"Once upon a time, there was a little girl named Lily who loved to dance in the rain.\"\n",
    "continuation_1 = generate_continuation(custom_first_1)\n",
    "\n",
    "custom_first_2 = \"Tim got a new robot toy for his birthday and was very excited to show it to his friends.\"\n",
    "continuation_2 = generate_continuation(custom_first_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tinystoriesproject-PsthS1z7-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
